---
title: UMAP Explained
layout: ./Layout.svelte
---

```js exec
import Figure from "./Figure.svelte";
import CechVisualization from "../figures/cech_visualization/components/Figure.svelte";
import FmnistVisualization from "../figures/fmnist_visualization/components/Figure.svelte";
import MammothUmapVisualization from "../figures/mammoth_visualization/components/FigureUmap.svelte";
import MammothTsneVisualization from "../figures/mammoth_visualization/components/FigureTsne.svelte";
import ToyVisualization from "../figures/toy_visualization/components/Figure.svelte";
```

# { \_metadata.title }

Dimensionality reduction is a key tool in the toolbox of data scientists and machine learning practitioners. Most commonly used for visualization, dimensionality reduction offers a powerful window into understanding large high-dimensional datasets.

UMAP (Universal Manifold Approximation and Projection) is a new technique by McInnes et. al that offers a number of advantages over other widely used algorithms such as t-SNE. First of all, UMAP is fast, scaling well in terms of both dataset size and dimensionality. UMAP is also better than t-SNE at preserving global structure of the data. Finally, UMAP is built on strong theoretical foundations that allow more advanced functionality and more understandable hyperparameters.

<Figure>
  <ToyVisualization />
  <span slot="caption">
    Apply UMAP projection to various toy datasets. Note that UMAP's performance is relatively unaffected by the dataset size and dimensionality.
  </span>
</Figure>

## A quick dip into UMAP theory

UMAP is, at its core, a graph layout algorithm that works very similarly to t-SNE. Essentially, UMAP constructs a weighted graph from the high dimensional data, with edge strength representing how close a given point is to its neighbor. It then optimizes a lower-dimensional graph to be as similar as possible in structure to the original, high-dimensional graph.

<Figure>
  <FmnistVisualization />
  <span slot="caption">
    UMAP projection applied to the Fashion MNIST dataset. 28x28 images of clothing items in 10 categories are encoded as 784-dimensional vectors and then projected to 3 using UMAP. Notice how well clustered each different category is (local structure), while similar categories tend to colocate together (global structure).
  </span>
</Figure>

## UMAP Hyperparameters

UMAP offers a number of hyperparameters that are much more understandable than the `perplexity` hyperparameter in t-SNE. We’ll consider two of the more commonly used hyperparameters below.

The most basic hyperparameter is `n_neighbors` - the number of approximate nearest neighbors used to construct the initial high-dimensional graph. It effectively controls how UMAP balances local versus global structure - low values will push UMAP to focus more on local structure by constraining the number of neighboring points considered when analyzing the data in high dimensions, while high values will push UMAP towards representing the big-picture structure while losing fine detail.

The second hyperparameter we’ll investigate is `min_dist`, or the minimum distance apart that points are allowed to be in low-dimensional space. This parameter controls how tightly UMAP clumps points together, with low values leading to more clumped embeddings. Larger values of `min_dist` will make UMAP pack points together less, focusing instead on the preservation of the broad topological structure.

The following visualization - extended from excellent work by Max Noichl - is an exploration of the impact of UMAP hyperparameters on a 2D projection of 3D data. By changing the `n_neighbors` and `min_dist` hyperparameters, you can explore their impact on the resulting dimensionality reduction.

<Figure>
  <MammothUmapVisualization />
  <span slot="caption">
    UMAP projections of a 3D woolly mammoth skeleton into 2 dimensions, with various settings for the `n_neighbors` and `min_dist` hyperparameters.
  </span>
</Figure>

While most applications of UMAP involve projection from high-dimensional data, the projection from 3D serves as a useful analogy to understand how UMAP prioritizes global vs local structure depending on its hyperparameters. As `n_neighbors` increases, UMAP connects more and more neighboring points when constructing the graph representation of the high-dimensional data, which leads to a projection that more accurately reflects the global structure of the data. At very low values, any notion of global structure is almost completely lost. As the `min_dist` parameter increases, UMAP tends to "spread out" the projected points, leading to decreased clustering of the data and less emphasis on global structure.

## UMAP vs t-SNE

UMAP provides a number of advantages over t-SNE, including increased speed and performance that scales much better with dataset size and dimensionality. However, the most important difference between the two algorithms is in how they balance between local and global structure - UMAP is much better at preserving global structure in the final projection. This means that the inter-cluster relations are potentially more meaningful than in t-SNE (however, any given axis or distance in lower dimensions still isn’t interpretable in the way of techniques such as PCA).

<Figure>
  <MammothTsneVisualization />
  <span slot="caption">
    A comparison between UMAP and t-SNE projections of a 3D woolly mammoth skeleton into 2 dimensions, with various settings for hyperparameters. Notice how much more global structure is preserved with UMAP, particularly with larger values of nNeighbors.
  </span>
</Figure>

For almost every value of the `perplexity` hyperparameter, t-SNE tends to "spread out" the projected data with very little preservation of the global structure. In contrast, UMAP tends to group adjacent pieces of the higher-dimensional structure together in low dimensions, which reflects an increased preservation of global structure. It's also notable that t-SNE projections vary widely from run to run, with different pieces of the higher-dimensional data projected to wildly different locations. While UMAP is also a stochastic algorithm, it's striking how similar the resulting projections are from run to run and with different hyperparameters. This is due, again, to UMAP's increased emphasis on global structure in comparison to t-SNE.

## Conclusion

UMAP is an incredibly powerful tool in the data scientist's arsenal, and

## A deeper dive into UMAP theory

Most dimensionality reduction algorithms fit into either one of two broad categories: Matrix factorization (such as PCA) or Graph layout (such as t-SNE). At its core, UMAP is a graph layout algorithm, very similar to t-SNE, but with a number of key theoretical underpinnings that give the algorithm a more solid footing.

In its most simple sense, the UMAP algorithm consists of two steps: construction of a graph in high dimensions followed by an optimization step to find the most similar graph in lower dimensions. In order to achieve this goal, the algorithm relies on a number of insights from algebraic topology and Riemannian geometry. Despite the intimidating mathematics, the intuitions behind the core principles are actually quite simple: UMAP essentially constructs a weighted graph from the high dimensional data, with edge strength representing how “close” a given point is to another, then projects this graph down to a lower dimensionality. The advanced mathematics gives UMAP a solid footing with which to handle the challenges of doing this in high dimensions with real data.

In order to construct the initial high-dimensional graph, UMAP relies on constructing what’s known as a **Čech complex**, which is a way of representing a topology combinatorially (using sets rather than continuous geometry). In order to get there, we’ll use a basic building block called a **simplex**. Geometrically, a simplex is a k-dimensional object formed by connecting k + 1 points - for example, a 0-simplex is a point, a 1-simplex is a line, and a 2-simplex is a triangle. By thinking of our data as a set of simplices, we can capture a representation of the topology, and by combining those simplices in a specific way to form a Čech complex, we get some theoretical guarantees about how well it represents the topology.

We begin by considering each point in our data as a sample from a continuous, high-dimensional shape (our topology). We can think of each point as a 0-simplex. By extending out from each point some radius r, and connecting points that overlap, we can construct sets of 1-, 2-, and higher-dimensional simplices. This simplicial complex does a reasonable job of approximating the fundamental topology of the dataset, and by considering just the 0- and 1-simplices, we’ve effectively just constructed a graph, which can be readily projected into a lower-dimensional analogue.

Unfortunately, real-world high-dimensional data presents a problem that UMAP needs to overcome - picking the right sized radius. Too small a radius and we’ll tend towards isolated, local clusters of points. Too large, and everything becomes connected. This problem is exacerbated by the **curse of dimensionality**, where distances between points become increasingly similar in higher dimensions. UMAP solves this problem in a clever way: Rather than using a fixed radius, UMAP uses a variable radius determined for each point based on the distance to its kth nearest neighbor. Within this local radius, connectedness is then made “fuzzy” by making each connection a probability, with further points less likely to be connected. Since we don’t want any points to be completely isolated, a constraint is added that all points must be connected to at least its closest neighboring point. The final output of this process is a weighted graph, with edge weights representing the likelihood that two points are “connected” in our high-dimensional manifold.

The following interactive example demonstrates a rough version of how UMAP constructs the simplicial complex from a toy dataset.

<Figure>
  <CechVisualization />
  <span slot="caption">
    Adjust the slider to extend a radius outwards from each point, computed by the distance to its nth nearest neighbor. Notice that past the intersection with the first neighbor, the radius begins to get fuzzy, with subsequent connections appearing with less weight;
  </span>
</Figure>

Note that since each point’s local notion of distance may be different than its neighbors’, we must resolve whether two points are connected based on potentially different directed edge weights. UMAP squares this inconsistency by computing the probability that at least one of the edges exist.

Once the final, fuzzy simplicial complex is constructed, UMAP projects the data into lower dimensions essentially via a force-directed graph layout algorithm. This optimization step is actually very similar to t-SNE, but by jumping through the theoretical hoops while constructing our initial simplicial complex, UMAP is able to accelerate the optimization and preserve much more global structure than t-SNE.
